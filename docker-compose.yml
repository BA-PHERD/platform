x-kafka-shared-image: &kafka-shared-image
  image: 'bitnami/kafka:3.6.2'

# cannot keep image and healtcheck together because the kafka-init does not need a healthcheck
x-kafka-shared-healthcheck: &kafka-shared-healthcheck
  healthcheck:
    test: ["CMD-SHELL", "kafka-topics.sh --list --bootstrap-server kafka-broker-0:9092 #,kafka-broker-1:9092" ]
    interval: 5s
    timeout: 5s
    retries: 10
    start_period: 2m
    # start_interval: 10s

x-kafka-shared-envs: &kafka-shared-envs
  KAFKA_CFG_PROCESS_ROLES: controller,broker
  KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
  KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
  KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka-broker-0:9093 #,1@kafka-broker-1:9093
  KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
  KAFKA_KRAFT_CLUSTER_ID: clusterid0with016bytes

services:
  data-loading-webapp:
    build:
      context: edge-vm
      dockerfile: ./Dockerfile
    ports:
      - "${DATA_LOADING_WEBAPP_PORT}:80"
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker-0:9092 #,kafka-broker-1:9092
      - OWNER_ID=test-user

# ----------------------------- KAFKA -----------------------------
# https://github.com/bitnami/containers/blob/466fffb6f2fe8cef54f281f22e62a967f7d69b78/bitnami/kafka/README.md

  kafka-broker-0:
    <<: [*kafka-shared-image, *kafka-shared-healthcheck]
    environment:
      <<: *kafka-shared-envs
      KAFKA_CFG_NODE_ID: 0
    # volumes:
    #   - ./kafka/data0:/bitnami/kafka

  # kafka-broker-1:
  #   <<: [*kafka-shared-image, *kafka-shared-healthcheck]
  #   environment:
  #     <<: *kafka-shared-envs
  #     KAFKA_CFG_NODE_ID: 1

  kafka-init:
    <<: *kafka-shared-image
    depends_on:
      kafka-broker-0:
        condition: service_healthy
      # kafka-broker-1:
      #   condition: service_healthy
    entrypoint:
      - 'bash'
      - '-c'
      - |
        kafka-topics.sh --create \
          --if-not-exists \
          --topic "devprin.patients" \
          --replication-factor=1 \
          --bootstrap-server kafka-broker-0:9092 && \
        kafka-topics.sh --create \
          --if-not-exists \
          --topic "devprin.mir-results" \
          --replication-factor=1 \
          --bootstrap-server kafka-broker-0:9092 #,kafka-broker-1:9092

# ----------------------------- HIVE METASTORE -----------------------------
# https://github.com/naushadh/hive-metastore
  # hive-metastore:
  #   image: naushadh/hive-metastore
  #   depends_on:
  #     ranger-db:
  #       condition: service_healthy
  #     minio-init:
  #       condition: service_completed_successfully
  #   environment:
  #     - DATABASE_HOST=ranger-db
  #     - DATABASE_DB=hive_metastore_db
  #     - DATABASE_USER=${HIVE_METASTORE_DATABASE_USER}
  #     - DATABASE_PASSWORD=${HIVE_METASTORE_DATABASE_PASSWORD}
  #     - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
  #     - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
  #     - S3_ENDPOINT_URL=http://minio:9000
  #     - S3_BUCKET=prin
  #     - S3_PREFIX=metadata
  #   # ports:
  #   #   - '9083:9083'

  hive-metastore:
    build:
      args:
        HIVE_VERSION: ${HIVE_VERSION}
      context: ./hive/docker
    image: hive-with-postgres:${HIVE_VERSION}
    depends_on:
      ranger-db:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "cat /proc/net/tcp6 | sed -n '2p' | awk '{ print $$2 }' | grep 237B"]
      start_period: 1m
      retries: 5
      timeout: 5s
      interval: 5s
    environment:
      - IS_RESUME=${SKIP_HIVE_SCHEMA_INIT:-false}
      - SERVICE_NAME=metastore
      - HIVE_CUSTOM_CONF_DIR=/opt/hive/custom-conf
      - DB_DRIVER=postgres
      - DB_DRIVER_CLASSNAME=org.postgresql.Driver
      - DB_CONNECTION_URL=jdbc:postgresql://ranger-db:5432/hive_metastore_db
      - DB_USER=${HIVE_METASTORE_DATABASE_USER}
      - DB_PASSWORD=${HIVE_METASTORE_DATABASE_PASSWORD}
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - S3_ENDPOINT_URL=http://minio:9000
      - S3_BUCKET=prin
      - S3_PREFIX=metadata
    volumes:
      - ./hive/conf:/opt/hive/custom-conf
      - ./hive/hooks/atlas/import-hive.sh:/opt/hive/import-hive.sh
    # ports:
    #   - '9083:9083'


# ----------------------------- MINIO -----------------------------
  minio:
    image: quay.io/minio/minio:latest
    ports:
    #   - '9000:9000'
      - '${MINIO_PORT}:9001'
    # volumes:
    #   - './minio/data:/data'
    environment:
      - MINIO_ROOT_USER
      - MINIO_ROOT_PASSWORD
    healthcheck:
      test: curl --fail http://localhost:9000/minio/health/live
      start_period: 1m
      retries: 10
      timeout: 5s
      interval: 5s
    command: ["server", "/data", "--console-address", ":9001"]

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
        mc alias set local_minio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}
        mc rb --force local_minio/prin
        mc mb local_minio/prin

# ----------------------------- TRINO -----------------------------
# https://trino.io/docs/current/installation/containers.html

  trino:
    build:
      context: ./trino/docker/server
      args:
        TRINO_VERSION: ${TRINO_VERSION}
    stdin_open: true
    tty: true
    ports:
      - "${TRINO_PORT}:8080"
    healthcheck:
      test: ["CMD-SHELL", "/usr/lib/trino/bin/health-check"]
      interval: 10s
      start_period: 2m
      timeout: 5s
      retries: 10
    depends_on:
      ranger:
        condition: service_healthy
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/kafka-description-files:/etc/trino/kafka
      - ./trino/configs/config.properties:/etc/trino/config.properties
      - ./trino/configs/password-authenticator.properties:/etc/trino/password-authenticator.properties
      - ./trino/configs/password.db:/etc/trino/sec-conf/password.db
      - ./trino/https/server.pem:/etc/trino/sec-conf/server.pem
      - ./trino/plugins/ranger/access-control.properties:/etc/trino/access-control.properties
      - ./trino/plugins/ranger/ranger-policymgr-ssl.xml:/etc/trino/ranger-policymgr-ssl.xml
      - ./trino/plugins/ranger/ranger-trino-audit.xml:/etc/trino/ranger-trino-audit.xml
      - ./trino/plugins/ranger/ranger-trino-security.xml:/etc/trino/ranger-trino-security.xml

# ----------------------------- CDC -----------------------------
## Includes the Trino CLI. This container encompasses a CDC script
## that copies the new Kafka messages to a Trino table (pointing to an S3 bucket)

  cdc-script:
    build:
      context: ./trino/docker/cdc
      args:
        TRINO_VERSION: ${TRINO_VERSION}
    image: trino-cdc:${TRINO_VERSION}
    environment:
      - TRINO_ENDPOINT=http://trino:8080
      - FLUSH_DELAY_SECONDS=15
    depends_on:
      trino:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    volumes:
      - ./trino/docker/cdc/scripts:/cdc/scripts
      - ./trino/docker/cdc/data:/cdc/data

# ----------------------------- RANGER -----------------------------

  ranger:
    image: apache/ranger:${RANGER_VERSION}
    stdin_open: true
    tty: true
    ports:
      - "${RANGER_PORT}:6080"
    depends_on:
      ranger-zk:
        condition: service_started
      ranger-db:
        condition: service_healthy
      ranger-solr:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "http://localhost:6080", "--timeout", "5", "--spider", "--quiet"]
      interval: 10s
      start_period: 2m
      timeout: 5s
      retries: 10
    volumes:
      - ./ranger/install.properties:/opt/ranger/admin/install.properties
    environment:
      - DEBUG_ADMIN=${DEBUG_ADMIN:-false}
      - RANGER_VERSION
      - RANGER_DB_TYPE=postgres
    command:
      - /home/ranger/scripts/ranger.sh

  ranger-zk:
    image: apache/ranger-zk:${RANGER_VERSION}
    # ports:
    #   - "2181:2181"

  ranger-solr:
    image: apache/ranger-solr:${RANGER_VERSION}
    # ports:
    #   - "8983:8983"
    volumes:
      - ./solr/data:/var/solr
    command:
      - solr-precreate
      - ranger_audits
      - /opt/solr/server/solr/configsets/ranger_audits/

  ranger-db:
    image: apache/ranger-db:${RANGER_VERSION}
    volumes:
      - ./postgres/data:/var/lib/postgresql/data
      - ./postgres/init/hive-metastore-init.sql:/docker-entrypoint-initdb.d/hive-metastore-init.sql
    healthcheck:
      test: 'su -c "pg_isready -q" postgres'
      interval: 10s
      timeout: 2s
      retries: 3

  # ranger-tagsync:
  #   build:
  #     context: ranger/tagsync/docker
  #     args:
  #       - TAGSYNC_VERSION=${RANGER_VERSION}
  #   image: ranger-tagsync
  #   depends_on:
  #     ranger:
  #       condition: service_started
  #   volumes:
  #     - ./ranger/tagsync/configs/ranger-tagsync-install.properties:/opt/ranger/tagsync/install.properties
  #     - ./ranger/tagsync/configs/ranger-tagsync-tags.json:/opt/ranger/tagsync/data/tags.json
  #   environment:
  #     - TAGSYNC_VERSION=${RANGER_VERSION}
  #     - DEBUG_TAGSYNC=${DEBUG_TAGSYNC:-false}

# ----------------------------- ATLAS  -----------------------------
## https://hub.docker.com/r/sburn/apache-atlas

  atlas:
    image: sburn/apache-atlas:2.3.0
    ports:
      - "${ATLAS_PORT}:21000"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:21000/login.jsp"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      # zookeeper:
      #   condition: service_healthy
    volumes:
      - ./atlas/conf/atlas-application.properties:/apache-atlas/conf/atlas-application.properties
      - ./atlas/conf/users-credentials.properties:/apache-atlas/conf/users-credentials.properties
      - ./atlas/data:/apache-atlas/data
      - ./atlas/logs:/apache-atlas/logs

  # zookeeper:
  #   image: zookeeper:3.9.3
  #   environment:
  #     - ZOO_4LW_COMMANDS_WHITELIST=*
  #   healthcheck:
  #     test: ["CMD", "echo", "ruok", "|", "nc", "-w", "2", "localhost", "2181"]
  #     interval: 20s
  #     timeout: 10s
  #     retries: 5

# ----------------------------- HUE -----------------------------
## https://hub.docker.com/r/gethue/hue
  
  # hue:
  #   image: gethue/hue:20240530-140101
  #   ports:
  #     - "${HUE_PORT}:8888"
  #   depends_on:
  #     trino:
  #       condition: service_healthy
  #   volumes:
  #     - ./hue/hue.ini:/usr/share/hue/desktop/conf/hue.ini
  
# ----------------------------- SUPERSET -----------------------------
## https://hub.docker.com/r/apache/superset

  superset:
    build:
      context: ./superset
      args:
        SUPERSET_VERSION: ${SUPERSET_VERSION}
    image: superset-with-trino:${SUPERSET_VERSION}
    ports:
      - "${SUPERSET_PORT}:8088"
    depends_on:
      trino:
        condition: service_healthy
    environment:
      - SUPERSET_SECRET_KEY=supersetkey
      - ADMIN_USERNAME=${SUPERSET_ADMIN_USERNAME}
      - ADMIN_PASSWORD=${SUPERSET_ADMIN_PASSWORD}
      - ADMIN_EMAIL=admin@superset.com
    volumes:
      - ./superset/bootstrap.sh:/bootstrap.sh
    entrypoint: ["/bootstrap.sh"]
